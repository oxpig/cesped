<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>cesped.datamanager.plDataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cesped.datamanager.plDataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
from os import PathLike

import torch
from torch.utils.data import DataLoader, BatchSampler, Sampler, RandomSampler
from typing import Union, Literal, Optional, Tuple, Iterable

from lightning import pytorch as pl

from cesped.constants import defaultBenchmarkDir
from cesped.particlesDataset import ParticlesDataset

from cesped.datamanager.augmentations import Augmenter

class ParticlesDataModule(pl.LightningDataModule):
    &#34;&#34;&#34;
    ParticlesDataModule: A LightningDataModule that wraps a ParticlesDataset
    &#34;&#34;&#34;

    def __init__(self, targetName: Union[PathLike, str], halfset: Literal[0, 1], image_size: int,
                 benchmarkDir: str = defaultBenchmarkDir, apply_perImg_normalization: bool = True,
                 ctf_correction: Literal[&#34;none&#34;, &#34;phase_flip&#34;] = &#34;phase_flip&#34;, image_size_factor_for_crop: float = 0.25,
                 num_augmented_copies_per_batch: int = 2,
                 augmenter: Optional[Augmenter] = None, train_validaton_split_seed: int = 113,
                 train_validation_split: Tuple[float, float] = (0.7, 0.3), batch_size: int = 8,
                 num_data_workers: int = 0):
        &#34;&#34;&#34;
        ##Builder

        Args:
            targetName (Union[PathLike, str]): The name of the target to use. It is also the basename of \
            the directory where the data is.
            halfset (Literal[0, 1]): The second parameter.
            image_size (bool): The final size of the image (after cropping).
            benchmarkDir (str): The root directory where the datasets are downloaded.
            apply_perImg_normalization (bool): Apply cryo-EM per-image normalization. I = (I-noiseMean)/noiseStd.
            ctf_correction (Literal[none, phase_flip]): phase_flip will correct amplitude inversion due to defocus
            image_size_factor_for_crop (float): Fraction of the image size to be cropped. Final size of the image \
            is origSize*(1-image_size_factor_for_crop). It is important because particles in cryo-EM tend to \
            be only 50% to 25% of the total area of the image.
            augmenter (Augmenter): A data augmentator object to be applied to the training dataloader. If none, data won&#39;t be augmented
            train_validaton_split_seed (int): The train/validation seed used for random split
            train_validation_split (Tuple[float]): The fraction of the dateset that should be split for train and for validation
            batch_size (int): The batch size
            num_data_workers (int): The number of workers for data loading. Set it to 0 to use the same thread as the model

        &#34;&#34;&#34;

        super().__init__()
        # self.save_hyperparameters()  #Not needed since we are using CLI
        self.targetName = targetName
        self.halfset = halfset
        self.benchmarkDir = os.path.expanduser(benchmarkDir)
        self.image_size = image_size
        self.apply_perImg_normalization = apply_perImg_normalization
        self.ctf_correction = ctf_correction
        self.image_size_factor_for_crop = image_size_factor_for_crop
        self.num_augmented_copies_per_batch = num_augmented_copies_per_batch
        self.augmenter = augmenter
        self.train_validaton_split_seed = train_validaton_split_seed
        self.train_validation_split = train_validation_split
        self.batch_size = batch_size
        self.num_data_workers = num_data_workers

        self._symmetry = None

    @property
    def symmetry(self):
        &#34;&#34;&#34;The point symmetry of the dataset&#34;&#34;&#34;
        if self._symmetry is None:
            dataset = self.createDataset()
            self._symmetry = dataset.symmetry
        return self._symmetry

    def createDataset(self):
        return ParticlesDataset(self.targetName, halfset=self.halfset, benchmarkDir=self.benchmarkDir,
                                image_size=self.image_size, apply_perImg_normalization=self.apply_perImg_normalization)
                                
    def _create_dataloader(self, partitionName: Optional[str]):

        dataset = self.createDataset()
        if partitionName in [&#34;train&#34;, &#34;val&#34;]:
            assert self.train_validation_split is not None, &#34;Error, self.train_validation_split required&#34;
            dataset.augmenter = self.augmenter if partitionName == &#34;train&#34; else None
            generator = torch.Generator().manual_seed(self.train_validaton_split_seed) #This is new
            train_dataset, val_dataset = torch.utils.data.random_split(dataset, self.train_validation_split,
                                                                       generator=generator)

            if partitionName == &#34;train&#34;:
                dataset = train_dataset
                print(f&#34;Train dataset {len(train_dataset)}&#34;)

                batch_sampler = MultiInstanceSampler(sampler=RandomSampler(dataset), batch_size=self.batch_size,
                                                     drop_last=True,
                                                     num_copies_to_sample=self.num_augmented_copies_per_batch)
                return DataLoader(
                                dataset,
                                batch_sampler=batch_sampler,
                                num_workers=self.num_data_workers,
                                persistent_workers=True if self.num_data_workers &gt; 0 else False)
            else:
                dataset = val_dataset
                print(f&#34;Validation dataset {len(val_dataset)}&#34;)

        return DataLoader(
            dataset, batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_data_workers,
            persistent_workers=True if self.num_data_workers &gt; 0 else False)

    def train_dataloader(self):
        return self._create_dataloader(partitionName=&#34;train&#34;)

    def val_dataloader(self):
        return self._create_dataloader(partitionName=&#34;val&#34;)

    def test_dataloader(self):
        return self._create_dataloader(partitionName=&#34;test&#34;)

    def predict_dataloader(self):
        return self._create_dataloader(partitionName=None)


class MultiInstanceSampler(BatchSampler):
    def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool,
                 num_copies_to_sample:int=1):
        assert  batch_size % num_copies_to_sample == 0, &#34;Error, batch_size % num_copies_to_sample == 0 required&#34;
        super().__init__(sampler, batch_size//num_copies_to_sample, drop_last)
        self.num_copies_to_sample = num_copies_to_sample

    def __iter__(self):
        for idx in super(MultiInstanceSampler, self).__iter__():
            yield idx * self.num_copies_to_sample</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cesped.datamanager.plDataset.MultiInstanceSampler"><code class="flex name class">
<span>class <span class="ident">MultiInstanceSampler</span></span>
<span>(</span><span>sampler: Union[torch.utils.data.sampler.Sampler[int], Iterable[int]], batch_size: int, drop_last: bool, num_copies_to_sample: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps another sampler to yield a mini-batch of indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampler</code></strong> :&ensp;<code>Sampler</code> or <code>Iterable</code></dt>
<dd>Base sampler. Can be any iterable object</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of mini-batch.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code></dt>
<dd>If <code>True</code>, the sampler will drop the last batch if
its size would be less than <code>batch_size</code></dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiInstanceSampler(BatchSampler):
    def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool,
                 num_copies_to_sample:int=1):
        assert  batch_size % num_copies_to_sample == 0, &#34;Error, batch_size % num_copies_to_sample == 0 required&#34;
        super().__init__(sampler, batch_size//num_copies_to_sample, drop_last)
        self.num_copies_to_sample = num_copies_to_sample

    def __iter__(self):
        for idx in super(MultiInstanceSampler, self).__iter__():
            yield idx * self.num_copies_to_sample</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.sampler.BatchSampler</li>
<li>torch.utils.data.sampler.Sampler</li>
<li>typing.Generic</li>
</ul>
</dd>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule"><code class="flex name class">
<span>class <span class="ident">ParticlesDataModule</span></span>
<span>(</span><span>targetName: Union[os.PathLike, str], halfset: Literal[0, 1], image_size: int, benchmarkDir: str = '/home/sanchezg/tmp/cryoSupervisedDataset', apply_perImg_normalization: bool = True, ctf_correction: Literal['none', 'phase_flip'] = 'phase_flip', image_size_factor_for_crop: float = 0.25, num_augmented_copies_per_batch: int = 2, augmenter: Optional[<a title="cesped.datamanager.augmentations.Augmenter" href="augmentations.html#cesped.datamanager.augmentations.Augmenter">Augmenter</a>] = None, train_validaton_split_seed: int = 113, train_validation_split: Tuple[float, float] = (0.7, 0.3), batch_size: int = 8, num_data_workers: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>ParticlesDataModule: A LightningDataModule that wraps a ParticlesDataset</p>
<h2 id="builder">Builder</h2>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>targetName</code></strong> :&ensp;<code>Union[PathLike, str]</code></dt>
<dd>The name of the target to use. It is also the basename of
the directory where the data is.</dd>
<dt><strong><code>halfset</code></strong> :&ensp;<code>Literal[0, 1]</code></dt>
<dd>The second parameter.</dd>
<dt><strong><code>image_size</code></strong> :&ensp;<code>bool</code></dt>
<dd>The final size of the image (after cropping).</dd>
<dt><strong><code>benchmarkDir</code></strong> :&ensp;<code>str</code></dt>
<dd>The root directory where the datasets are downloaded.</dd>
<dt><strong><code>apply_perImg_normalization</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply cryo-EM per-image normalization. I = (I-noiseMean)/noiseStd.</dd>
<dt><strong><code>ctf_correction</code></strong> :&ensp;<code>Literal[none, phase_flip]</code></dt>
<dd>phase_flip will correct amplitude inversion due to defocus</dd>
<dt><strong><code>image_size_factor_for_crop</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of the image size to be cropped. Final size of the image
is origSize*(1-image_size_factor_for_crop). It is important because particles in cryo-EM tend to
be only 50% to 25% of the total area of the image.</dd>
<dt><strong><code>augmenter</code></strong> :&ensp;<code>Augmenter</code></dt>
<dd>A data augmentator object to be applied to the training dataloader. If none, data won't be augmented</dd>
<dt><strong><code>train_validaton_split_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>The train/validation seed used for random split</dd>
<dt><strong><code>train_validation_split</code></strong> :&ensp;<code>Tuple[float]</code></dt>
<dd>The fraction of the dateset that should be split for train and for validation</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The batch size</dd>
<dt><strong><code>num_data_workers</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of workers for data loading. Set it to 0 to use the same thread as the model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ParticlesDataModule(pl.LightningDataModule):
    &#34;&#34;&#34;
    ParticlesDataModule: A LightningDataModule that wraps a ParticlesDataset
    &#34;&#34;&#34;

    def __init__(self, targetName: Union[PathLike, str], halfset: Literal[0, 1], image_size: int,
                 benchmarkDir: str = defaultBenchmarkDir, apply_perImg_normalization: bool = True,
                 ctf_correction: Literal[&#34;none&#34;, &#34;phase_flip&#34;] = &#34;phase_flip&#34;, image_size_factor_for_crop: float = 0.25,
                 num_augmented_copies_per_batch: int = 2,
                 augmenter: Optional[Augmenter] = None, train_validaton_split_seed: int = 113,
                 train_validation_split: Tuple[float, float] = (0.7, 0.3), batch_size: int = 8,
                 num_data_workers: int = 0):
        &#34;&#34;&#34;
        ##Builder

        Args:
            targetName (Union[PathLike, str]): The name of the target to use. It is also the basename of \
            the directory where the data is.
            halfset (Literal[0, 1]): The second parameter.
            image_size (bool): The final size of the image (after cropping).
            benchmarkDir (str): The root directory where the datasets are downloaded.
            apply_perImg_normalization (bool): Apply cryo-EM per-image normalization. I = (I-noiseMean)/noiseStd.
            ctf_correction (Literal[none, phase_flip]): phase_flip will correct amplitude inversion due to defocus
            image_size_factor_for_crop (float): Fraction of the image size to be cropped. Final size of the image \
            is origSize*(1-image_size_factor_for_crop). It is important because particles in cryo-EM tend to \
            be only 50% to 25% of the total area of the image.
            augmenter (Augmenter): A data augmentator object to be applied to the training dataloader. If none, data won&#39;t be augmented
            train_validaton_split_seed (int): The train/validation seed used for random split
            train_validation_split (Tuple[float]): The fraction of the dateset that should be split for train and for validation
            batch_size (int): The batch size
            num_data_workers (int): The number of workers for data loading. Set it to 0 to use the same thread as the model

        &#34;&#34;&#34;

        super().__init__()
        # self.save_hyperparameters()  #Not needed since we are using CLI
        self.targetName = targetName
        self.halfset = halfset
        self.benchmarkDir = os.path.expanduser(benchmarkDir)
        self.image_size = image_size
        self.apply_perImg_normalization = apply_perImg_normalization
        self.ctf_correction = ctf_correction
        self.image_size_factor_for_crop = image_size_factor_for_crop
        self.num_augmented_copies_per_batch = num_augmented_copies_per_batch
        self.augmenter = augmenter
        self.train_validaton_split_seed = train_validaton_split_seed
        self.train_validation_split = train_validation_split
        self.batch_size = batch_size
        self.num_data_workers = num_data_workers

        self._symmetry = None

    @property
    def symmetry(self):
        &#34;&#34;&#34;The point symmetry of the dataset&#34;&#34;&#34;
        if self._symmetry is None:
            dataset = self.createDataset()
            self._symmetry = dataset.symmetry
        return self._symmetry

    def createDataset(self):
        return ParticlesDataset(self.targetName, halfset=self.halfset, benchmarkDir=self.benchmarkDir,
                                image_size=self.image_size, apply_perImg_normalization=self.apply_perImg_normalization)
                                
    def _create_dataloader(self, partitionName: Optional[str]):

        dataset = self.createDataset()
        if partitionName in [&#34;train&#34;, &#34;val&#34;]:
            assert self.train_validation_split is not None, &#34;Error, self.train_validation_split required&#34;
            dataset.augmenter = self.augmenter if partitionName == &#34;train&#34; else None
            generator = torch.Generator().manual_seed(self.train_validaton_split_seed) #This is new
            train_dataset, val_dataset = torch.utils.data.random_split(dataset, self.train_validation_split,
                                                                       generator=generator)

            if partitionName == &#34;train&#34;:
                dataset = train_dataset
                print(f&#34;Train dataset {len(train_dataset)}&#34;)

                batch_sampler = MultiInstanceSampler(sampler=RandomSampler(dataset), batch_size=self.batch_size,
                                                     drop_last=True,
                                                     num_copies_to_sample=self.num_augmented_copies_per_batch)
                return DataLoader(
                                dataset,
                                batch_sampler=batch_sampler,
                                num_workers=self.num_data_workers,
                                persistent_workers=True if self.num_data_workers &gt; 0 else False)
            else:
                dataset = val_dataset
                print(f&#34;Validation dataset {len(val_dataset)}&#34;)

        return DataLoader(
            dataset, batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_data_workers,
            persistent_workers=True if self.num_data_workers &gt; 0 else False)

    def train_dataloader(self):
        return self._create_dataloader(partitionName=&#34;train&#34;)

    def val_dataloader(self):
        return self._create_dataloader(partitionName=&#34;val&#34;)

    def test_dataloader(self):
        return self._create_dataloader(partitionName=&#34;test&#34;)

    def predict_dataloader(self):
        return self._create_dataloader(partitionName=None)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>lightning.pytorch.core.datamodule.LightningDataModule</li>
<li>lightning.pytorch.core.hooks.DataHooks</li>
<li>lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.symmetry"><code class="name">var <span class="ident">symmetry</span></code></dt>
<dd>
<div class="desc"><p>The point symmetry of the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def symmetry(self):
    &#34;&#34;&#34;The point symmetry of the dataset&#34;&#34;&#34;
    if self._symmetry is None:
        dataset = self.createDataset()
        self._symmetry = dataset.symmetry
    return self._symmetry</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.createDataset"><code class="name flex">
<span>def <span class="ident">createDataset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createDataset(self):
    return ParticlesDataset(self.targetName, halfset=self.halfset, benchmarkDir=self.benchmarkDir,
                            image_size=self.image_size, apply_perImg_normalization=self.apply_perImg_normalization)</code></pre>
</details>
</dd>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.predict_dataloader"><code class="name flex">
<span>def <span class="ident">predict_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying prediction samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~lightning.pytorch.trainer.trainer.Trainer.predict</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="return">Return</h2>
<p>A :class:<code>torch.utils.data.DataLoader</code> or a sequence of them specifying prediction samples.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dataloader(self):
    return self._create_dataloader(partitionName=None)</code></pre>
</details>
</dd>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying test samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~lightning.pytorch.trainer.trainer.Trainer.test</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a test dataset and a :meth:<code>test_step</code>, you don't need to implement
this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dataloader(self):
    return self._create_dataloader(partitionName=&#34;test&#34;)</code></pre>
</details>
</dd>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~lightning.pytorch.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~lightning.pytorch.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return self._create_dataloader(partitionName=&#34;train&#34;)</code></pre>
</details>
</dd>
<dt id="cesped.datamanager.plDataset.ParticlesDataModule.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~lightning.pytorch.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~lightning.pytorch.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~lightning.pytorch.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return self._create_dataloader(partitionName=&#34;val&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cesped.datamanager" href="index.html">cesped.datamanager</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cesped.datamanager.plDataset.MultiInstanceSampler" href="#cesped.datamanager.plDataset.MultiInstanceSampler">MultiInstanceSampler</a></code></h4>
</li>
<li>
<h4><code><a title="cesped.datamanager.plDataset.ParticlesDataModule" href="#cesped.datamanager.plDataset.ParticlesDataModule">ParticlesDataModule</a></code></h4>
<ul class="two-column">
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.createDataset" href="#cesped.datamanager.plDataset.ParticlesDataModule.createDataset">createDataset</a></code></li>
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.predict_dataloader" href="#cesped.datamanager.plDataset.ParticlesDataModule.predict_dataloader">predict_dataloader</a></code></li>
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.symmetry" href="#cesped.datamanager.plDataset.ParticlesDataModule.symmetry">symmetry</a></code></li>
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.test_dataloader" href="#cesped.datamanager.plDataset.ParticlesDataModule.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.train_dataloader" href="#cesped.datamanager.plDataset.ParticlesDataModule.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="cesped.datamanager.plDataset.ParticlesDataModule.val_dataloader" href="#cesped.datamanager.plDataset.ParticlesDataModule.val_dataloader">val_dataloader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>